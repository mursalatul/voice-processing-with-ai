{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "import numpy as np\n",
    "import librosa\n",
    "import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(np.__version__)\n",
    "\n",
    "# all emotions on RAVDESS dataset\n",
    "int2emotion = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "# we allow only these emotions\n",
    "AVAILABLE_EMOTIONS = {\n",
    "    \"angry\",\n",
    "    \"sad\",\n",
    "    \"neutral\",\n",
    "    \"happy\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "def extract_feature(file_name, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract feature from audio file `file_name`\n",
    "        Features supported:\n",
    "            - MFCC (mfcc)\n",
    "            - Chroma (chroma)\n",
    "            - MEL Spectrogram Frequency (mel)\n",
    "            - Contrast (contrast)\n",
    "            - Tonnetz (tonnetz)\n",
    "        e.g:\n",
    "        `features = extract_feature(path, mel=True, mfcc=True)`\n",
    "    \"\"\"\n",
    "    mfcc = kwargs.get(\"mfcc\")\n",
    "    chroma = kwargs.get(\"chroma\")\n",
    "    mel = kwargs.get(\"mel\")\n",
    "    contrast = kwargs.get(\"contrast\")\n",
    "    tonnetz = kwargs.get(\"tonnetz\")\n",
    "    \n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        \n",
    "        if chroma or contrast:\n",
    "            stft = np.abs(librosa.stft(X))\n",
    "        \n",
    "        result = np.array([])\n",
    "\n",
    "        def ensure_1d(feature, name):\n",
    "            if feature.ndim > 1:\n",
    "                print(f\"{name} original shape: {feature.shape}\")\n",
    "                feature = np.mean(feature, axis=1)\n",
    "                print(f\"{name} shape after mean: {feature.shape}\")\n",
    "            return feature.flatten()\n",
    "\n",
    "        if mfcc:\n",
    "            mfccs = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40)\n",
    "            mfccs_flat = ensure_1d(mfccs, \"MFCCs\")\n",
    "            result = np.hstack((result, mfccs_flat))\n",
    "        \n",
    "        if chroma:\n",
    "            chroma = librosa.feature.chroma_stft(S=stft, sr=sample_rate)\n",
    "            chroma_flat = ensure_1d(chroma, \"Chroma\")\n",
    "            result = np.hstack((result, chroma_flat))\n",
    "        \n",
    "        if mel:\n",
    "            mel = librosa.feature.melspectrogram(y=X, sr=sample_rate)\n",
    "            mel_flat = ensure_1d(mel, \"MEL\")\n",
    "            result = np.hstack((result, mel_flat))\n",
    "        \n",
    "        if contrast:\n",
    "            contrast = librosa.feature.spectral_contrast(S=stft, sr=sample_rate)\n",
    "            contrast_flat = ensure_1d(contrast, \"Contrast\")\n",
    "            result = np.hstack((result, contrast_flat))\n",
    "        \n",
    "        if tonnetz:\n",
    "            tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate)\n",
    "            tonnetz_flat = ensure_1d(tonnetz, \"Tonnetz\")\n",
    "            result = np.hstack((result, tonnetz_flat))\n",
    "    \n",
    "    print(f\"Final feature shape: {result.shape}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m):\n\u001b[0;32m      6\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Programming_related\\PROJECTS\\ALL_PROJECT\\voice-processing-with-ai\\.conda\\lib\\site-packages\\sklearn\\__init__.py:81\u001b[0m\n\u001b[0;32m     72\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartial import of sklearn during the build process.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# in utils.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# (OpenMP is loaded by importing show_versions right after this block)\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthreadpoolctl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ThreadpoolController\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(test_size=0.2):\n",
    "    X, y = [], []\n",
    "    files = glob.glob(r\"\\Actors_1\\Actor_*\\*.wav\")  # Update this path\n",
    "    print(\"Files found:\", len(files))\n",
    "    \n",
    "    for file in files:\n",
    "        # get the base name of the audio file\n",
    "        basename = os.path.basename(file)\n",
    "        # get the emotion label\n",
    "        emotion = int2emotion.get(basename.split(\"-\")[2])\n",
    "        print(f\"Processing file: {file}, Emotion: {emotion}\")\n",
    "        \n",
    "        # we allow only AVAILABLE_EMOTIONS we set\n",
    "        if emotion in AVAILABLE_EMOTIONS:\n",
    "            # extract speech features\n",
    "            features = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "            print(f\"Extracted features shape: {features.shape}\")\n",
    "            # add to data\n",
    "            X.append(features)\n",
    "            y.append(emotion)\n",
    "        else:\n",
    "            print(f\"Skipping emotion: {emotion}\")\n",
    "    \n",
    "    if not X:\n",
    "        raise ValueError(\"No data loaded - check file paths and emotion filtering.\")\n",
    "    \n",
    "    # split the data to training and testing and return it\n",
    "    return train_test_split(np.array(X), y, test_size=test_size, random_state=7)\n",
    "\n",
    "# Try to load the data\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test = load_data(test_size=0.25)\n",
    "    print(\"Data loaded successfully\")\n",
    "    print(\"[+] Number of training samples:\", len(X_train))\n",
    "    print(\"[+] Number of testing samples:\", len(X_test))\n",
    "except ValueError as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = load_data(test_size=0.25)\n",
    "# Try to load the data\n",
    "# try:\n",
    "#     X_train, X_test, y_train, y_test = load_data(test_size=0.25)\n",
    "#     print(\"Data loaded successfully\")\n",
    "#     print(\"[+] Number of training samples:\", len(X_train))\n",
    "#     print(\"[+] Number of testing samples:\", len(X_test))\n",
    "# except ValueError as e:\n",
    "#     print(\"Error:\", e)\n",
    "print(type(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# print some details\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# number of samples in training data\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[+] Number of training samples:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# number of samples in testing data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[+] Number of testing samples:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# print some details\n",
    "# number of samples in training data\n",
    "print(\"[+] Number of training samples:\", X_train.shape[0])\n",
    "# number of samples in testing data\n",
    "print(\"[+] Number of testing samples:\", X_test.shape[0])\n",
    "# number of features used\n",
    "# this is a vector of features extracted\n",
    "# using utils.extract_features() method\n",
    "print(\"[+] Number of features:\", X_train.shape[1])\n",
    "# best model, determined by a grid search\n",
    "model_params = {\n",
    "    \"alpha\": 0.01,\n",
    "    \"batch_size\": 256,\n",
    "    \"epsilon\": 1e-08,\n",
    "    \"hidden_layer_sizes\": (300,),\n",
    "    \"learning_rate\": \"adaptive\",\n",
    "    \"max_iter\": 500,\n",
    "}\n",
    "# initialize Multi Layer Perceptron classifier\n",
    "# with best parameters ( so far )\n",
    "model = MLPClassifier(**model_params)\n",
    "\n",
    "# train the model\n",
    "print(\"[*] Training the model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict 25% of data to measure how good we are\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# calculate the accuracy\n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# now we save the model\n",
    "# make result directory if doesn't exist yet\n",
    "if not os.path.isdir(\"result\"):\n",
    "    os.mkdir(\"result\")\n",
    "\n",
    "pickle.dump(model, open(\"result/mlp_classifier.model\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
